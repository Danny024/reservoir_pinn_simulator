{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Reservoir PINN Simulator\n",
    "\n",
    "This Jupyter Notebook explains the Python code implementation for a Physics-Informed Neural Network (PINN) that simulates pressure in a 3D reservoir. The code solves a 3D partial differential equation (PDE) for single-phase fluid flow, incorporating data from a CSV file, boundary conditions, and initial conditions. It as well includes static 2D slice visualizations and an animation of the pressure field.\n",
    "\n",
    "### Prerequisites\n",
    "- Python 3.8+\n",
    "- Required libraries: `torch`, `numpy`, `pandas`, `matplotlib`, `ffmpeg-python`\n",
    "- FFmpeg installed for animation rendering\n",
    "- A sample `data/reservoir_data.csv` file with columns `x`, `y`, `z`, `t`, `p` (provided with 50 points)\n",
    "\n",
    "### Overview\n",
    "The code:\n",
    "1. Defines reservoir parameters and a PINN model for 3D input (x, y, z, t).\n",
    "2. Computes the PDE residual for the 3D flow equation.\n",
    "3. Loads sparse data from `data/reservoir_data.csv`.\n",
    "4. Generates collocation, boundary, and initial condition points in 3D.\n",
    "5. Trains the PINN using Adam and L-BFGS optimizers.\n",
    "6. Validates the model and visualizes results (2D slices and animation).\n",
    "7. Plots the PINN architecture.\n",
    "\n",
    "The PDE is:\n",
    "\n",
    "$$\n",
    "\\phi\\,c_t\\,\\frac{\\partial p}{\\partial t}\n",
    "\\;-\\;\n",
    "\\frac{k}{\\mu}\\Biggl(\n",
    "  \\frac{\\partial^2 p}{\\partial x^2}\n",
    "  + \\frac{\\partial^2 p}{\\partial y^2}\n",
    "  + \\frac{\\partial^2 p}{\\partial z^2}\n",
    "\\Biggr)\n",
    "\\;=\\;q\n",
    "$$\n",
    "\n",
    "where the source term is\n",
    "\n",
    "$$\n",
    "q = q_0 \\exp\\!\\Biggl(\n",
    "  -\\frac{(x - x_w)^2 + (y - y_w)^2 + (z - z_w)^2}{2\\,\\sigma^2}\n",
    "\\Biggr).\n",
    "$$\n",
    "\n",
    "\n",
    "### Boundary Conditions (BC)\n",
    "\n",
    "No-flow (Neumann) conditions are applied on the domain boundaries ($x=0,1$, $y=0,1$, $z=0,1$):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial p}{\\partial n} = 0\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Initial Conditions (IC)\n",
    "\n",
    "A uniform pressure is set at $t=0$:\n",
    "\n",
    "$$\n",
    "p(x, y, z, 0) = p_{\\text{base}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries\n",
    "\n",
    "This cell imports the necessary libraries for numerical computations, neural networks, data handling, visualization, and animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/daniel/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: numpy in /home/daniel/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/daniel/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /home/daniel/anaconda3/lib/python3.12/site-packages (3.10.1)\n",
      "Collecting ffmpeg-python\n",
      "  Using cached ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/daniel/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/daniel/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/daniel/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/daniel/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/daniel/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/daniel/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/daniel/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/daniel/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/daniel/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/daniel/anaconda3/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/daniel/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/daniel/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Collecting future (from ffmpeg-python)\n",
      "  Using cached future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/daniel/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/daniel/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Using cached ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Using cached future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Installing collected packages: future, ffmpeg-python\n",
      "Successfully installed ffmpeg-python-0.2.0 future-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch numpy pandas matplotlib ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, Image\n",
    "import base64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set Random Seed and Device\n",
    "\n",
    "This cell ensures reproducibility by setting random seeds and configures the device (GPU or CPU) for computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define Reservoir Parameters\n",
    "\n",
    "This cell defines the physical parameters of the 3D reservoir and the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reservoir parameters\n",
    "phi = 0.2  # Porosity\n",
    "k = 0.1  # Permeability (Darcy)\n",
    "mu = 1.0  # Viscosity (cP)\n",
    "ct = 1e-6  # Total compressibility (1/Pa)\n",
    "q0 = 0.01  # Injection rate (m^3/s)\n",
    "well_pos = (0.5, 0.5, 0.5)  # Well location in 3D\n",
    "p_base = 1000.0  # Base pressure (Pa)\n",
    "p_scale = 1000.0  # Scale for normalization\n",
    "\n",
    "# Explanation:\n",
    "# - phi: Porosity, fraction of pore volume (0.2).\n",
    "# - k: Permeability, ease of fluid flow (0.1 Darcy).\n",
    "# - mu: Fluid viscosity (1.0 cP).\n",
    "# - ct: Total compressibility (1e-6 1/Pa), accounts for fluid and rock.\n",
    "# - q0: Injection rate at the well (0.01 m^3/s).\n",
    "# - well_pos: Coordinates of the injection well (0.5, 0.5, 0.5) in 3D.\n",
    "# - p_base: Initial/base pressure (1000 Pa).\n",
    "# - p_scale: Scaling factor for pressure normalization (1000 Pa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define PINN Model\n",
    "\n",
    "This cell defines the PINN neural network architecture with 8 layers, accepting 4 inputs (x, y, z, t)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network architecture (8 layers, 200 neurons)\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PINN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(4, 200),  # 4 inputs: x, y, z, t\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x) * p_scale + p_base  # Scale and shift output\n",
    "\n",
    "# Explanation:\n",
    "# - PINN: Inherits from nn.Module, defining a neural network.\n",
    "# - __init__: Constructs an 8-layer network:\n",
    "#   - Input layer: 4 inputs (x, y, z, t) for 3D spatial and temporal coordinates.\n",
    "#   - 6 hidden layers: 200 neurons each, with Tanh activation.\n",
    "#   - Output layer: 1 output (pressure p).\n",
    "# - forward: Computes the network output, scaling by p_scale (1000) and shifting by p_base (1000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Compute PDE Residual\n",
    "\n",
    "This cell defines a function to compute the residual of the 3D PDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute PDE residual\n",
    "def compute_pde_residual(model, x, y, z, t):\n",
    "    inputs = torch.stack([x, y, z, t], dim=1).requires_grad_(True).to(device)\n",
    "    p = model(inputs)\n",
    "    \n",
    "    # Compute gradients\n",
    "    p_grad = torch.autograd.grad(p, inputs, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
    "    p_x, p_y, p_z, p_t = p_grad[:, 0], p_grad[:, 1], p_grad[:, 2], p_grad[:, 3]\n",
    "    \n",
    "    # Second derivatives\n",
    "    p_xx = torch.autograd.grad(p_x, inputs, grad_outputs=torch.ones_like(p_x), create_graph=True)[0][:, 0]\n",
    "    p_yy = torch.autograd.grad(p_y, inputs, grad_outputs=torch.ones_like(p_y), create_graph=True)[0][:, 1]\n",
    "    p_zz = torch.autograd.grad(p_z, inputs, grad_outputs=torch.ones_like(p_z), create_graph=True)[0][:, 2]\n",
    "    \n",
    "    # Source term\n",
    "    sigma = 0.05\n",
    "    q = q0 * torch.exp(-((x - well_pos[0])**2 + (y - well_pos[1])**2 + (z - well_pos[2])**2) / (2 * sigma**2))\n",
    "    \n",
    "    # PDE: phi * ct * dp/dt - (k/mu) * (d^2p/dx^2 + d^2p/dy^2 + d^2p/dz^2) = q\n",
    "    residual = phi * ct * p_t - (k / mu) * (p_xx + p_yy + p_zz) - q\n",
    "    return residual\n",
    "\n",
    "# Explanation:\n",
    "# - compute_pde_residual: Computes the residual of the 3D PDE.\n",
    "# - inputs: Combines x, y, z, t into a tensor, with requires_grad=True for differentiation.\n",
    "# - p: Predicted pressure from the PINN model.\n",
    "# - p_grad: Computes first derivatives (p_x, p_y, p_z, p_t) using autograd.\n",
    "# - p_xx, p_yy, p_zz: Computes second derivatives for the 3D Laplacian.\n",
    "# - q: 3D Gaussian source term centered at well_pos.\n",
    "# - residual: PDE residual, minimized to enforce the physical law."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Load Sparse Dataset\n",
    "\n",
    "This cell loads the 3D pressure data from `data/reservoir_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sparse dataset from CSV\n",
    "try:\n",
    "    data_df = pd.read_csv(\"data/reservoir_data.csv\")\n",
    "    if not all(col in data_df.columns for col in ['x', 'y', 'z', 't', 'p']):\n",
    "        raise ValueError(\"CSV file must contain columns: x, y, z, t, p\")\n",
    "    data = torch.tensor(data_df[['x', 'y', 'z', 't', 'p']].values, dtype=torch.float32).to(device)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'data/reservoir_data.csv' not found. Please provide the CSV file.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV file: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "x_d, y_d, z_d, t_d, p_d = data[:, 0], data[:, 1], data[:, 2], data[:, 3], data[:, 4]\n",
    "N_d = len(data)\n",
    "\n",
    "# Explanation:\n",
    "# - data_df: Reads 'data/reservoir_data.csv' with 50 points (5 original, 45 augmented).\n",
    "# - Validation: Ensures columns x, y, z, t, p exist.\n",
    "# - data: Converts DataFrame to a PyTorch tensor, moved to device.\n",
    "# - x_d, y_d, z_d, t_d, p_d: Extracts 3D coordinates, time, and pressure.\n",
    "# - N_d: Number of data points (50).\n",
    "# - Error handling: Exits on file not found or invalid format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Generate Collocation Points\n",
    "\n",
    "This cell generates 3D points for PDE, boundary, and initial conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate collocation points with adaptive sampling\n",
    "N_r = 20000  # Collocation points\n",
    "N_b = 2000   # Boundary points\n",
    "N_i = 2000   # Initial condition points\n",
    "N_r_adaptive = 10000  # Adaptive points\n",
    "\n",
    "# Random collocation points\n",
    "x_r = torch.rand(N_r).to(device)\n",
    "y_r = torch.rand(N_r).to(device)\n",
    "z_r = torch.rand(N_r).to(device)\n",
    "t_r = torch.rand(N_r).to(device)\n",
    "\n",
    "# Adaptive sampling near well position\n",
    "x_r_adaptive = torch.normal(mean=0.5, std=0.1, size=(N_r_adaptive,)).clamp(0, 1).to(device)\n",
    "y_r_adaptive = torch.normal(mean=0.5, std=0.1, size=(N_r_adaptive,)).clamp(0, 1).to(device)\n",
    "z_r_adaptive = torch.normal(mean=0.5, std=0.1, size=(N_r_adaptive,)).clamp(0, 1).to(device)\n",
    "t_r_adaptive = torch.rand(N_r_adaptive).to(device)\n",
    "\n",
    "x_r = torch.cat([x_r, x_r_adaptive])\n",
    "y_r = torch.cat([y_r, y_r_adaptive])\n",
    "z_r = torch.cat([z_r, z_r_adaptive])\n",
    "t_r = torch.cat([t_r, t_r_adaptive])\n",
    "\n",
    "# Boundary points (faces of the cube)\n",
    "x_b = torch.cat([torch.zeros(N_b//6), torch.ones(N_b//6), torch.rand(N_b//3), torch.rand(N_b//3)]).to(device)\n",
    "y_b = torch.cat([torch.rand(N_b//3), torch.rand(N_b//3), torch.zeros(N_b//6), torch.ones(N_b//6)]).to(device)\n",
    "z_b = torch.cat([torch.rand(N_b//3), torch.rand(N_b//3), torch.rand(N_b//3), torch.rand(N_b//3)]).to(device)\n",
    "t_b = torch.rand(N_b).to(device)\n",
    "\n",
    "# Initial condition points\n",
    "x_i = torch.rand(N_i).to(device)\n",
    "y_i = torch.rand(N_i).to(device)\n",
    "z_i = torch.rand(N_i).to(device)\n",
    "t_i = torch.zeros(N_i).to(device)\n",
    "\n",
    "# Explanation:\n",
    "# - N_r, N_b, N_i, N_r_adaptive: Number of points for PDE, boundaries, initial conditions, and adaptive sampling.\n",
    "# - Collocation points: Randomly sampled in [0,1]^3 for x, y, z, and t for PDE enforcement.\n",
    "# - Adaptive points: Sampled near well (0.5, 0.5, 0.5) to focus on injection region.\n",
    "# - Boundary points: Sampled on cube faces (x=0,1; y=0,1; z=0,1) with random t.\n",
    "# - Initial condition points: Random x, y, z with t=0.\n",
    "# - All tensors are moved to device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Initialize Model and Optimizer\n",
    "\n",
    "This cell initializes the PINN model and sets up the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "model = PINN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=2000, gamma=0.5)\n",
    "\n",
    "# Explanation:\n",
    "# - model: Instantiates the 3D PINN model and moves it to device.\n",
    "# - optimizer: Uses Adam optimizer with learning rate 0.001.\n",
    "# - scheduler: Reduces learning rate by half every 2000 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Training Loop (Adam)\n",
    "\n",
    "This cell trains the PINN using Adam, minimizing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 17.31 MiB is free. Process 555264 has 138.00 MiB memory in use. Process 555688 has 5.73 GiB memory in use. Including non-PyTorch memory, this process has 1.72 GiB memory in use. Of the allocated memory 1.47 GiB is allocated by PyTorch, and 77.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# PDE residual loss\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m residual \u001b[38;5;241m=\u001b[39m compute_pde_residual(model, x_r, y_r, z_r, t_r)\n\u001b[1;32m      8\u001b[0m loss_pde \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(residual\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Boundary condition loss\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m, in \u001b[0;36mcompute_pde_residual\u001b[0;34m(model, x, y, z, t)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Second derivatives\u001b[39;00m\n\u001b[1;32m     11\u001b[0m p_xx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(p_x, inputs, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(p_x), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 12\u001b[0m p_yy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(p_y, inputs, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(p_y), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     13\u001b[0m p_zz \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(p_z, inputs, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(p_z), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Source term\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    493\u001b[0m         grad_outputs_\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m _engine_run_backward(\n\u001b[1;32m    497\u001b[0m         outputs,\n\u001b[1;32m    498\u001b[0m         grad_outputs_,\n\u001b[1;32m    499\u001b[0m         retain_graph,\n\u001b[1;32m    500\u001b[0m         create_graph,\n\u001b[1;32m    501\u001b[0m         inputs,\n\u001b[1;32m    502\u001b[0m         allow_unused,\n\u001b[1;32m    503\u001b[0m         accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    504\u001b[0m     )\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    509\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 7.68 GiB of which 17.31 MiB is free. Process 555264 has 138.00 MiB memory in use. Process 555688 has 5.73 GiB memory in use. Including non-PyTorch memory, this process has 1.72 GiB memory in use. Of the allocated memory 1.47 GiB is allocated by PyTorch, and 77.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Training loop (Adam)\n",
    "epochs = 20000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # PDE residual loss\n",
    "    residual = compute_pde_residual(model, x_r, y_r, z_r, t_r)\n",
    "    loss_pde = torch.mean(residual**2)\n",
    "    \n",
    "    # Boundary condition loss\n",
    "    inputs_b = torch.stack([x_b, y_b, z_b, t_b], dim=1).requires_grad_(True).to(device)\n",
    "    p_b = model(inputs_b)\n",
    "    p_grad_b = torch.autograd.grad(p_b, inputs_b, grad_outputs=torch.ones_like(p_b), create_graph=True)[0]\n",
    "    p_x_b, p_y_b, p_z_b = p_grad_b[:, 0], p_grad_b[:, 1], p_grad_b[:, 2]\n",
    "    \n",
    "    mask_x0 = (x_b == 0)\n",
    "    mask_x1 = (x_b == 1)\n",
    "    loss_bc_x = torch.mean(p_x_b[mask_x0]**2) + torch.mean(p_x_b[mask_x1]**2)\n",
    "    \n",
    "    mask_y0 = (y_b == 0)\n",
    "    mask_y1 = (y_b == 1)\n",
    "    loss_bc_y = torch.mean(p_y_b[mask_y0]**2) + torch.mean(p_y_b[mask_y1]**2)\n",
    "    \n",
    "    mask_z0 = (z_b == 0)\n",
    "    mask_z1 = (z_b == 1)\n",
    "    loss_bc_z = torch.mean(p_z_b[mask_z0]**2) + torch.mean(p_z_b[mask_z1]**2)\n",
    "    \n",
    "    loss_bc = loss_bc_x + loss_bc_y + loss_bc_z\n",
    "    \n",
    "    # Initial condition loss\n",
    "    inputs_i = torch.stack([x_i, y_i, z_i, t_i], dim=1).to(device)\n",
    "    p_i = model(inputs_i)\n",
    "    loss_ic = torch.mean(((p_i - p_base) / p_scale)**2)\n",
    "    \n",
    "    # Data loss\n",
    "    inputs_d = torch.stack([x_d, y_d, z_d, t_d], dim=1).to(device)\n",
    "    p_d_pred = model(inputs_d)\n",
    "    loss_data = torch.mean(((p_d_pred - p_d) / p_scale)**2)\n",
    "    \n",
    "    # Total loss\n",
    "    loss = loss_pde + 100 * loss_bc + 10000 * loss_ic + 1000000 * loss_data\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Residual-based adaptive sampling\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        residual_abs = torch.abs(residual)\n",
    "        top_indices = torch.topk(residual_abs, k=1000).indices\n",
    "        x_r = torch.cat([x_r, x_r[top_indices]])\n",
    "        y_r = torch.cat([y_r, y_r[top_indices]])\n",
    "        z_r = torch.cat([z_r, z_r[top_indices]])\n",
    "        t_r = torch.cat([t_r, t_r[top_indices]])\n",
    "    \n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            p_d_pred = model(inputs_d)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}, \"\n",
    "                  f\"PDE: {loss_pde.item():.6f}, BC: {loss_bc.item():.6f}, \"\n",
    "                  f\"IC: {loss_ic.item():.6f}, Data: {loss_data.item():.6f}\")\n",
    "            print(\"Predicted vs Actual pressures:\")\n",
    "            for i in range(N_d):\n",
    "                print(f\"Point {i+1}: Predicted={p_d_pred[i].item():.2f}, Actual={p_d[i].item():.2f}\")\n",
    "\n",
    "# Explanation of Cell:\n",
    "# - epochs: Trains for 20,000 iterations.\n",
    "# - loss_pde: Mean squared PDE residual in 3D.\n",
    "# - loss_bc: Enforces no-flow conditions on all cube faces.\n",
    "# - loss_ic: Enforces initial pressure p_base.\n",
    "# - loss_data: Matches predictions to the 50-point dataset.\n",
    "# - loss: Weighted sum of losses to balance contributions.\n",
    "# - Adaptive sampling: Adds points with high residual every 1000 epochs.\n",
    "# - Logging: Prints loss components and predictions every 1000 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. L-BFGS Optimization\n",
    "\n",
    "This cell refines the model using L-BFGS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L-BFGS optimization\n",
    "optimizer = torch.optim.LBFGS(model.parameters(), lr=0.1)\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    residual = compute_pde_residual(model, x_r, y_r, z_r, t_r)\n",
    "    loss_pde = torch.mean(residual**2)\n",
    "    inputs_b = torch.stack([x_b, y_b, z_b, t_b], dim=1).requires_grad_(True).to(device)\n",
    "    p_b = model(inputs_b)\n",
    "    p_grad_b = torch.autograd.grad(p_b, inputs_b, grad_outputs=torch.ones_like(p_b), create_graph=True)[0]\n",
    "    p_x_b, p_y_b, p_z_b = p_grad_b[:, 0], p_grad_b[:, 1], p_grad_b[:, 2]\n",
    "    loss_bc_x = torch.mean(p_x_b[(x_b == 0)]**2) + torch.mean(p_x_b[(x_b == 1)]**2)\n",
    "    loss_bc_y = torch.mean(p_y_b[(y_b == 0)]**2) + torch.mean(p_y_b[(y_b == 1)]**2)\n",
    "    loss_bc_z = torch.mean(p_z_b[(z_b == 0)]**2) + torch.mean(p_z_b[(z_b == 1)]**2)\n",
    "    loss_bc = loss_bc_x + loss_bc_y + loss_bc_z\n",
    "    inputs_i = torch.stack([x_i, y_i, z_i, t_i], dim=1).to(device)\n",
    "    p_i = model(inputs_i)\n",
    "    loss_ic = torch.mean(((p_i - p_base) / p_scale)**2)\n",
    "    inputs_d = torch.stack([x_d, y_d, z_d, t_d], dim=1).to(device)\n",
    "    p_d_pred = model(inputs_d)\n",
    "    loss_data = torch.mean(((p_d_pred - p_d) / p_scale)**2)\n",
    "    loss = loss_pde + 100 * loss_bc + 10000 * loss_ic + 1000000 * loss_data\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.step(closure)\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"L-BFGS Iteration {i+1}, Loss: {closure().item():.6f}\")\n",
    "\n",
    "# Explanation of Cell:\n",
    "# - optimizer: Uses L-BFGS with learning rate 0.1.\n",
    "# - closure: Computes the same loss as Adam, adapted for 3D.\n",
    "# - Training: Runs 100 iterations.\n",
    "# - Logging: Prints loss every 10 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Save the Model\n",
    "\n",
    "This cell saves the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'reservoir_pinn.pth')\n",
    "print(\"Model saved as 'reservoir_pinn.pth'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Validate Model\n",
    "\n",
    "This cell computes the MSE and compares predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation: Compute MSE and print predictions\n",
    "with torch.no_grad():\n",
    "    p_d_pred = model(inputs_d)\n",
    "    mse_data = torch.mean(((p_d_pred - p_d) / p_scale)**2).item()\n",
    "    print(f\"Final MSE on sparse data (normalized): {mse_data:.6f}\")\n",
    "    print(\"Predicted vs Actual pressures:\")\n",
    "    for i in range(N_d):\n",
    "        print(f\"Point {i+1}: Predicted={p_d_pred[i].item():.2f}, Actual={p_d[i].item():.2f}\")\n",
    "\n",
    "# Explanation of cells:\n",
    "# - torch.no_grad(): Disables gradient tracking.\n",
    "# - p_d_pred: Predicts pressure at 50 data points.\n",
    "# - mse_data: Computes normalized MSE.\n",
    "# - Prints MSE and predicted vs. actual pressures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Test Loading the Model\n",
    "\n",
    "This cell tests reloading the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading the saved model\n",
    "def load_and_predict(model_class, model_path, inputs, device):\n",
    "    model = model_class().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(inputs)\n",
    "    return predictions\n",
    "\n",
    "loaded_predictions = load_and_predict(PINN, 'reservoir_pinn.pth', inputs_d, device)\n",
    "print(\"Predictions from loaded model:\")\n",
    "for i in range(N_d):\n",
    "    print(f\"Point {i+1}: Loaded={loaded_predictions[i].item():.2f}, Actual={p_d[i].item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Visualize Pressure Field (Static)\n",
    "\n",
    "This cell plots a 2D pressure slice at a fixed z-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of pressure field (2D slice at z=0.5)\n",
    "def plot_pressure_slice(model, t_val, z_val, data_df):\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    y = np.linspace(0, 1, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.full_like(X, z_val)\n",
    "    T = np.full_like(X, t_val)\n",
    "    \n",
    "    inputs = torch.tensor(np.stack([X.flatten(), Y.flatten(), Z.flatten(), T.flatten()], axis=1), \n",
    "                         dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        P = model(inputs).cpu().numpy().reshape(X.shape)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(X, Y, P, levels=20, cmap=\"viridis\")\n",
    "    plt.colorbar(label=\"Pressure (Pa)\")\n",
    "    mask = np.abs(data_df['z'] - z_val) < 0.1  # Points near z_val\n",
    "    plt.scatter(data_df['x'][mask], data_df['y'][mask], c=\"red\", label=\"Data points\")\n",
    "    plt.title(f\"Pressure at z={z_val:.2f}, t={t_val:.2f}\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"pressure_z_{z_val:.2f}_t_{t_val:.2f}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Plot slices at different times\n",
    "plot_pressure_slice(model, 0.0, 0.5, data_df)\n",
    "plot_pressure_slice(model, 0.5, 0.5, data_df)\n",
    "plot_pressure_slice(model, 1.0, 0.5, data_df)\n",
    "\n",
    "# Explanation of cells:\n",
    "# - plot_pressure_slice: Plots a 2D pressure slice at fixed z and t.\n",
    "# - X, Y, Z, T: Meshgrid for x, y with constant z and t.\n",
    "# - inputs: 4D tensor for model prediction.\n",
    "# - P: Predicted pressures reshaped to 2D grid.\n",
    "# - Plot: Contour plot with data points near z_val, saved as PNG.\n",
    "# - Calls: Plots at t=0.0, 0.5, 1.0 for z=0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Visualize Pressure Field (Animation)\n",
    "\n",
    "This cell generates an animation of a 2D pressure slice over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def animate_pressure_slice(model, data_df, z_val, device, p_scale=1000.0, p_base=1000.0, filename='pressure_animation.gif'):\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    y = np.linspace(0, 1, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.full_like(X, z_val)\n",
    "    times = np.linspace(0, 1, 50)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    inputs = torch.tensor(np.stack([X.flatten(), Y.flatten(), Z.flatten(), np.zeros_like(X.flatten())], axis=1), \n",
    "                         dtype=torch.float32).to(device)\n",
    "\n",
    "    def update(frame):\n",
    "        ax.clear()\n",
    "        t_val = times[frame]\n",
    "        inputs[:, 3] = t_val\n",
    "        with torch.no_grad():\n",
    "            P = model(inputs, p_scale, p_base).cpu().numpy().reshape(X.shape)\n",
    "        contour = ax.contourf(X, Y, P, levels=20, cmap=\"viridis\")\n",
    "        mask = np.abs(data_df['z'] - z_val) < 0.1\n",
    "        ax.scatter(data_df['x'][mask], data_df['y'][mask], c=\"red\", label=\"Data points\")\n",
    "        ax.set_title(f\"Pressure at z={z_val:.2f}, t={t_val:.2f}\")\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"y\")\n",
    "        ax.legend()\n",
    "        if frame == 0:\n",
    "            fig.colorbar(contour, ax=ax, label=\"Pressure (Pa)\")\n",
    "        return contour,\n",
    "\n",
    "    anim = FuncAnimation(fig, update, frames=len(times), interval=100, blit=False)\n",
    "    anim.save(filename, writer='pillow', fps=10)\n",
    "    plt.close(fig)\n",
    "    return filename\n",
    "\n",
    "# Generate and display animation\n",
    "animation_file = animate_pressure_slice(model, data_df, 0.5, device)\n",
    "with open(animation_file, 'rb') as f:\n",
    "    display(Image(data=f.read(), format='gif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
